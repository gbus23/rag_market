# ğŸ“Š RAG + Kafka Integration

Cette documentation explique comment fonctionne l'intÃ©gration de Kafka avec la RAG app.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  News Sources        â”‚
â”‚ (Finnhub, Scrapers)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â†’ ingest_news_finhub.py â”€â”€â”
           â”‚                            â”‚
           â””â”€â†’ ingest_news_scrapping.py â”¤
                                        â”‚
                                        â†“
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  Kafka Topic   â”‚
                                    â”‚  (news_raw)    â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â†“â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚                 â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â†“â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â†“â”€â”€â”€â”€â”€â”
                            â”‚  RAG App    â”‚   â”‚  Other      â”‚
                            â”‚  (Mistral)  â”‚   â”‚  Consumers  â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Mode de fonctionnement

### 1. Mode Fichier (Par dÃ©faut)

```bash
python3 rag_app.py
```

- Charge articles depuis `ingestion/articles/*.json`
- Pas de dÃ©pendances Kafka
- Utile pour le dÃ©veloppement/test

### 2. Mode Kafka

```bash
# DÃ©marrer Kafka et les ingestors
kafka-storage format -t rZEz8XaLQCquqKoseif6kQ -c /opt/homebrew/etc/kafka/server.properties
kafka-broker-api.sh -c /opt/homebrew/etc/kafka/server.properties

python3 -m ingestion.ingest_news_finhub &
python3 -m ingestion.ingest_news_scrapping &

# DÃ©marrer la RAG app en mode Kafka
USE_KAFKA=true python3 rag_app.py
```

OU directement:

```bash
python3 rag_app_kafka.py
```

## Flux de donnÃ©es

### Ingestion (CÃ´tÃ© Producteur)

**ingest_news_finhub.py:**
```
Finnhub API
    â†“
fetch_news()
    â†“
normalize_article()  # Ajoute dÃ©tection ticker
    â†“
send_json(producer, NEWS_TOPIC, article)
    â†“
Kafka (topic: news_raw)
```

**ingest_news_scrapping.py:**
```
Web Scrapers (BFM, TradingView, Yahoo, etc.)
    â†“
run_once()
    â†“
row_to_event()  # DÃ©tecte ticker via resolve_ticker_from_text()
    â†“
send_json(producer, NEWS_TOPIC, event)
    â†“
Kafka (topic: news_raw)
```

### Consommation (CÃ´tÃ© RAG)

```
Kafka (topic: news_raw)
    â†“
KafkaArticleConsumer
    â”œâ”€ articles (deque de max 1000)
    â””â”€ articles_by_ticker (index par ticker)
    â†“
get_articles_by_ticker(ticker)
    â†“
RAGAppKafka.query()
    â†“
Format context + send to Mistral 3
    â†“
User answer
```

## Format des articles dans Kafka

Tous les articles envoyÃ©s ont ce format:

```json
{
  "id": "unique-id",
  "source": "Finnhub" | "BFM" | "TradingView" | "Yahoo",
  "headline": "Article title",
  "description": "Summary or snippet",
  "url": "https://...",
  "tickers": ["TSLA", "AAPL"],  // DÃ©tectÃ© automatiquement
  "published_at": "2025-12-10T15:30:00+00:00",
  "received_at": "2025-12-10T15:35:00+00:00",
  "raw": {
    // DonnÃ©es brutes originales
  }
}
```

## Configuration

### Variables d'environnement (.env)

```bash
# Kafka
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
NEWS_TOPIC=news_raw
PRICES_TOPIC=prices_raw

# APIs
FINNHUB_API_KEY=your_key
MARKETAUX_API_KEY=your_key
ALPHAVANTAGE_API_KEY=your_key
OPENFIGI_API_KEY=your_key

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
```

### Topics Kafka

- **news_raw** : Articles bruts des sources (Finnhub, scrapers)
- **prices_raw** : Prix des stocks (future)

## DÃ©tection des tickers

Les ingestors utilisent `resolve_ticker_from_text()` pour dÃ©tecter les tickers:

**RÃ¨gles:**
- DÃ©tecte 2-5 lettres majuscules + optionnellement `.X` (AAPL, GOOGL, BRK.B)
- Doit Ãªtre entourÃ© par dÃ©limiteur (espace, punctuation, guillemets, etc.)
- Ã‰vite faux positifs (ex: "EA" dans "I eat")

**PrioritÃ©:**
1. Tickers associÃ©s par Finnhub (si disponible)
2. DÃ©tection texte dans headline + summary

## Troubleshooting

### "Cannot connect to Kafka"
```bash
# VÃ©rifier Kafka
kafka-broker-api.sh -c /opt/homebrew/etc/kafka/server.properties

# VÃ©rifier topic
kafka-topics.sh --list --bootstrap-server localhost:9092
```

### Pas d'articles en consommation
1. VÃ©rifier ingestion est lancÃ©e:
   ```bash
   ps aux | grep ingest
   ```

2. VÃ©rifier messages dans Kafka:
   ```bash
   kafka-console-consumer.sh --bootstrap-server localhost:9092 \
     --topic news_raw --from-beginning --max-messages 5
   ```

### Articles pas filtrÃ©s par ticker
- VÃ©rifier que `resolve_ticker_from_text()` dÃ©tecte bien les tickers
- VÃ©rifier que l'article a le champ `"tickers"` non-vide

## Performance

- **Max articles en mÃ©moire:** 1000 (configurable)
- **Max articles par ticker:** 100
- **FenÃªtre de temps:** Derniers articles reÃ§us
- **Indexation:** Par ticker en temps rÃ©el

Pour gros volumes, considÃ©rer:
- Vector embeddings pour meilleure recherche
- Base de donnÃ©es pour persistance
- Multiple consumers pour parallÃ©lisation

## Prochaines Ã©tapes

- [ ] Persistance des articles en base de donnÃ©es
- [ ] Vector embeddings avec FAISS/Pinecone
- [ ] Multiple consumer groups
- [ ] Monitoring avec Prometheus
- [ ] Caching avec Redis
- [ ] Alertes sur mouvements de stock
- [ ] Multi-language support

---

Made with â¤ï¸ for real-time stock analysis
