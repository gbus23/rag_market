{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import threading\n",
    "from collections import deque\n",
    "from typing import Optional\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Reprendre les mÃªmes configs Azure que ci-dessus)\n",
    "AZURE_SEARCH_ENDPOINT = \"https://vectordbfundidcardfree.search.windows.net\"\n",
    "AZURE_SEARCH_KEY = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "INDEX_NAME = \"rag-index\"\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = \"https://alexa-me2hz4ri-swedencentral.cognitiveservices.azure.com/\"\n",
    "AZURE_OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "# Pour le LLM (GÃ©nÃ©ration) - On garde Ollama local ou on passe sur Azure GPT\n",
    "# Ici je garde Ollama pour minimiser les changements, mais on peut switcher.\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "MODEL_NAME = \"mistral\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _call_ollama(prompt):\n",
    "        # (MÃªme code que votre ancien RAGAppKafka)\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                f\"{OLLAMA_BASE_URL}/api/generate\",\n",
    "                json={\"model\": MODEL_NAME, \"prompt\": prompt, \"stream\": False}\n",
    "            )\n",
    "            return resp.json().get(\"response\", \"Erreur modÃ¨le\")\n",
    "        except Exception as e:\n",
    "            return f\"Erreur Ollama: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "load_dotenv()\n",
    "\n",
    "MODELS = {\n",
    "    \"gpt-4.1\": {\n",
    "        \"deployment\": \"gpt-4.1\",\n",
    "        \"endpoint\": \"https://al-mculmlvr-eastus2.cognitiveservices.azure.com/\",\n",
    "        \"api_version\": \"2024-12-01-preview\"\n",
    "    },\n",
    "    \"model-router\": {\n",
    "        \"deployment\": \"model-router\",\n",
    "        \"endpoint\": \"https://alexa-me2hz4ri-swedencentral.cognitiveservices.azure.com/\",\n",
    "        \"api_version\": \"2024-12-01-preview\"\n",
    "    },\n",
    "}\n",
    "\n",
    "SUBSCRIPTION_KEY = os.getenv(\"OPENAI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt_with_context(context: str, query: str, attachment: str, model_name=\"model-router\") -> str:\n",
    "    \"\"\"\n",
    "    Appelle Azure OpenAI en envoyant un prompt qui inclut :\n",
    "    - un contexte\n",
    "    - une requÃªte\n",
    "    - une piÃ¨ce attachÃ©e (par ex. transcript YouTube)\n",
    "    \"\"\"\n",
    "    config = MODELS.get(model_name)\n",
    "    if not config:\n",
    "        raise ValueError(f\"Model '{model_name}' not configured\")\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        api_version=config[\"api_version\"],\n",
    "        azure_endpoint=config[\"endpoint\"],\n",
    "        api_key=SUBSCRIPTION_KEY,\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an assistant.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    User request:\n",
    "    {query}\n",
    "\n",
    "    Attached document:\n",
    "    \\\"\\\"\\\"{attachment}\\\"\\\"\\\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=8192,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        model=config[\"deployment\"]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La finance est la discipline qui concerne la gestion de l'argent, incluant l'investissement, l'emprunt et l'Ã©pargne. Elle repose sur plusieurs concepts fondamentaux :\n",
      "\n",
      "1. **Les marchÃ©s financiers** : ce sont des lieux ou des plateformes oÃ¹ se rÃ©alisent l'achat et la vente d'instruments financiers. Ils permettent aux investisseurs de financer des entreprises ou de se constituer une Ã©pargne.\n",
      "\n",
      "2. **Les instruments financiers** : ce sont les produits Ã©changÃ©s sur les marchÃ©s, comme les actions, les obligations ou les dÃ©rivÃ©s. Ils servent Ã  investir, Ã  se couvrir contre les risques ou Ã  financer des projets.\n",
      "\n",
      "3. **La gestion des risques** : cela consiste Ã  identifier, Ã©valuer et maÃ®triser les risques liÃ©s aux investissements ou aux emprunts, afin de protÃ©ger les actifs et optimiser les rendements.\n",
      "\n",
      "En rÃ©sumÃ©, la finance inclut lâ€™Ã©tude des marchÃ©s, des produits financiers et des stratÃ©gies pour gÃ©rer efficacement lâ€™argent tout en minimisant les risques.\n"
     ]
    }
   ],
   "source": [
    "answer=call_gpt_with_context(\n",
    "    context=\"La finance est l'Ã©tude de la gestion de l'argent, y compris l'investissement, l'emprunt et l'Ã©pargne.\",\n",
    "    query=\"Explique-moi les bases de la finance.\",\n",
    "    attachment=\"La finance comprend des concepts tels que les marchÃ©s financiers, les instruments financiers, et la gestion des risques.\",\n",
    "    model_name=\"model-router\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGAppAzure:\n",
    "    def __init__(self):\n",
    "        # Clients Azure\n",
    "        self.search_client = SearchClient(\n",
    "            endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "            index_name=INDEX_NAME,\n",
    "            credential=AzureKeyCredential(AZURE_SEARCH_KEY)\n",
    "        )\n",
    "        self.openai_client = AzureOpenAI(\n",
    "            api_key=AZURE_OPENAI_KEY,\n",
    "            api_version=\"2024-12-01-preview\",\n",
    "            azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    "        )\n",
    "        \n",
    "        # Hack pour l'UI: on simule un buffer vide\n",
    "        self.buffer = DummyBuffer() \n",
    "        print(\"â˜ï¸ RAG App connectÃ©e Ã  Azure AI Search\")\n",
    "\n",
    "    def _generate_embedding(self, text):\n",
    "        response = self.openai_client.embeddings.create(input=[text], model=EMBEDDING_MODEL)\n",
    "        return response.data[0].embedding\n",
    "\n",
    "    def query(self, question: str) -> str:\n",
    "        print(f\"ðŸ” Question: {question}\")\n",
    "        \n",
    "        # 1. Vectorisation de la question\n",
    "        vector = self._generate_embedding(question)\n",
    "        \n",
    "        # 2. Recherche Vectorielle (Notebook logic)\n",
    "        vector_query = VectorizedQuery(vector=vector, k_nearest_neighbors=3, fields=\"contentVector\")\n",
    "        \n",
    "        results = self.search_client.search(\n",
    "            search_text=question, # Hybrid search (Text + Vector)\n",
    "            vector_queries=[vector_query],\n",
    "            select=[\"title\", \"content\", \"source\"],\n",
    "            top=3\n",
    "        )\n",
    "        \n",
    "        # 3. Construction du contexte\n",
    "        articles = list(results)\n",
    "        print(f\"ðŸ“„ Documents trouvÃ©s: {len(articles)}\")\n",
    "        \n",
    "        if not articles:\n",
    "            return \"DÃ©solÃ©, aucune information pertinente trouvÃ©e dans la base de connaissances.\"\n",
    "\n",
    "        context_str = \"\\n\\n\".join([\n",
    "            f\"- Titre: {doc['title']}\\n  Contenu: {doc['content']}\" \n",
    "            for doc in articles\n",
    "        ])\n",
    "        \n",
    "        # 4. GÃ©nÃ©ration (Via Ollama Local - comme avant)\n",
    "        prompt = f\"\"\"Tu es un analyste financier expert. Utilise UNIQUEMENT le contexte ci-dessous pour rÃ©pondre.\n",
    "        \n",
    "        Contexte:\n",
    "        {context_str}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        RÃ©ponse concise:\"\"\"\n",
    "        \n",
    "        return self._call_ollama(prompt)\n",
    "\n",
    "    def _call_ollama(self, prompt):\n",
    "        # (MÃªme code que votre ancien RAGAppKafka)\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                f\"{OLLAMA_BASE_URL}/api/generate\",\n",
    "                json={\"model\": MODEL_NAME, \"prompt\": prompt, \"stream\": False}\n",
    "            )\n",
    "            return resp.json().get(\"response\", \"Erreur modÃ¨le\")\n",
    "        except Exception as e:\n",
    "            return f\"Erreur Ollama: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test _call_ollama ===\n",
      "â˜ï¸ RAG App connectÃ©e Ã  Azure AI Search\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Test _call_ollama ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     test_call_ollama()\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Test query ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     test_query()\n",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m, in \u001b[0;36mtest_call_ollama\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m app \u001b[38;5;241m=\u001b[39m RAGAppAzure()\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplique-moi les bases de la finance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m app\u001b[38;5;241m.\u001b[39m_call_ollama(prompt)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest _call_ollama:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/RAG Market/rag_market/rag_app_azure.py:107\u001b[0m, in \u001b[0;36mRAGAppAzure._call_ollama\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_ollama\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# (MÃªme code que votre ancien RAGAppKafka)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m         resp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOLLAMA_BASE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m             json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: MODEL_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[1;32m    110\u001b[0m         )\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErreur modÃ¨le\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    645\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    646\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    647\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    648\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    649\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    650\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    651\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    652\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    653\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    654\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    655\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    656\u001b[0m     )\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    788\u001b[0m     conn,\n\u001b[1;32m    789\u001b[0m     method,\n\u001b[1;32m    790\u001b[0m     url,\n\u001b[1;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1423\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1422\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1423\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1425\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from rag_app_azure import RAGAppAzure\n",
    "\n",
    "def test_call_ollama():\n",
    "    app = RAGAppAzure()\n",
    "    prompt = \"Explique-moi les bases de la finance.\"\n",
    "    response = app._call_ollama(prompt)\n",
    "    print(\"Test _call_ollama:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"RÃ©ponse: {response}\")\n",
    "\n",
    "def test_query():\n",
    "    app = RAGAppAzure()\n",
    "    question = \"Pourquoi Tesla a-t-elle connu une telle volatilitÃ© rÃ©cemment ?\"\n",
    "    response = app.query(question)\n",
    "    print(\"Test query:\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"RÃ©ponse: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Test _call_ollama ===\")\n",
    "    test_call_ollama()\n",
    "    print(\"\\n=== Test query ===\")\n",
    "    test_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Azure contient 13 documents.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "search_client = SearchClient(\n",
    "            endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "            index_name=INDEX_NAME,\n",
    "            credential=AzureKeyCredential(AZURE_SEARCH_KEY))\n",
    "\n",
    "doc_count = search_client.get_document_count()\n",
    "print(f\"ðŸ“Š Azure contient {doc_count} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
